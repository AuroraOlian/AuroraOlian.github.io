# FDS

> the key to understand every data structure
> 1. äº†è§£æ¯ç§æ•°æ®ç»“æ„çš„å®šä¹‰æ–¹å¼
> 2. å„ç§æ“ä½œï¼Œex. å¢åˆ æ”¹æŸ¥ï¼Œæ ‡æ³¨å‡ºä¸€äº›å®¹æ˜“å¿˜è®°çš„(give pseudocode)
> 3. å¤æ‚åº¦åˆ†æï¼ï¼ˆæœ‰æ—¶å€™è¿™ä¸ªåˆ†æä¼šå˜å¾—å¾ˆå¤æ‚ï¼‰
> 4. collect wrong problem

> [!tldr] Link
> [other's error set](https://www.cnblogs.com/Sun-Wind/p/15771420.html)

## Stack æ ˆ

- [1] push & pop

> è‹¥ä¸€ä¸ªæ ˆçš„å…¥æ ˆåºåˆ—ä¸º 1ã€2ã€3ã€â€¦ã€Nï¼Œè¾“å‡ºåºåˆ—çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ iï¼Œåˆ™ç¬¬ j ä¸ªè¾“å‡ºå…ƒç´ æ˜¯

    max{1, i-j+1} ~ i+j-1?

- [2] åç¼€è¡¨è¾¾å¼
  `a*(b+c)-d` -> `a b c + * d -`

# Tree æ ‘

> [!quote]+ Conception
>
> - degree
>   - degree of a node: number of subtree
>   - degree of a tree: max{degree(node)}
> - parent/children
> - siblings/leaf
> - path
> - length of path ::= number of edges on the path
> - depth of $n_i$ ::= length of the unique path from the root to $n_i$
> - height
>
>   - height of $n_i$ ::= length of the longest path from $n_i$ to a leaf
>   - height of a tree ::=height(root) = depth(deepest leaf)
>
> - ancestors of a node ::= all the nodes along the path from the node up to the root
> - descendants of a node ::= all the nodes in its subtrees

> [!important] Type of Tree
>
> - [[#^ThreadedBinaryTree | Threaded Binary Tree]]
> - Binary Search Tree(BST)
> - Skewed Binary Trees
> - Complete Binary tree: All the leaf nodes are on two adjacent levels

> [!note] Properties of Binary Trees
>
> - The maximum number of nodes on levelÂ  iÂ  is $2^{i-1}$
> - The maximum number of nodes in a binary tree of depth k is $2^k - 1$
> - For any nonempty binary tree, $n_0 = n_2 + 1$ where $n_0$ is the number of leaf nodes and $n_2$ is the number of nodes of degree 2.
>   - (Generalized conclusion) For any tree, $n_0 = 1 + 0 \cdot n_1 + 1 \cdot n_2 + 2 \cdot n_3 + \cdots$ where $n_0$ is the number of leaf nodes, $n_i$ is the number of nodes of degree i(i > 0)

> [!note]- Threaded Binary Tree(çº¿ç´¢äºŒå‰æ ‘)
> [ä¸€ç¯‡ä¸é”™çš„æ–‡ç« ](https://blog.csdn.net/weixin_50502862/article/details/124662672)
> The links of a binary tree with n nodes are 2n, among which n+1 links are null. We can replace the null links by "threads" which will make traversals easier.
>
> - If Tree->Left is null, replace it with a pointer to the inorder predecessor of Tree
> - If Tree->Right is null, replace it with a pointer to the inorder successor of Tree.
>
> ```c
> typedefÂ  structÂ  ThreadedTreeNodeÂ  *PtrToÂ  ThreadedNode;
> typedefÂ  structÂ  PtrToThreadedNodeÂ  ThreadedTree;
> typedefÂ  structÂ  ThreadedTreeNode {
> Â Â Â Â Â Â  intÂ Â Â Â Â Â Â Â Â Â  Â  LeftThread;
> Â Â Â Â Â Â  ThreadedTreeÂ  Â  Left;
> Â Â Â Â Â Â  ElementTypeÂ  Element;
> Â Â Â Â Â Â  intÂ Â Â Â Â Â Â Â Â Â  Â  RightThread;
> Â Â Â Â Â   ThreadedTreeÂ  Â  Right;
> }
> ```

^ThreadedBinaryTree

> [!note] Complete Binary Tree
> We can represent a complete binary tree with n nodes sequentially, then for any node with index $i$, we have:
> `Array Representation :Â  BT [ n + 1 ]Â  ( BT [ 0 ] is not used)`
> index of parent(i) = $\lfloor \frac{i}{2} \rfloor(i \neq 1)$
> index of left child(i) = $2i(2i \le n)$
> index of right child(i) =$2i+1(2i+1 \le n)$

> [!summary]+ Operation
>
> - FirstChild-NextSibling Representation
>   - tree -> binary tree: å…ˆå…„å¼Ÿç›¸è¿ç•™é•¿å­ï¼Œè½¬ 45Â°ï¼Œ
>   - forest -> binary treeï¼šåœ¨æ¯ä¸ªæ ‘å½¢æˆå„è‡ªçš„äºŒå‰æ ‘çš„åŸºç¡€ä¸Šå°†æ ¹èŠ‚ç‚¹è¿èµ·æ¥
> - Pre-order/In-order/Post-order/Level-order Traversals (Recursively | **Iteratively**)
> - BST:
>   - Find/FindMin/FindMax
>   - Insert
>   - **Delete**
>     - Delete a leaf node:Â  Reset its parent link to NULL.
>     - Delete a degree 1 node:Â  Replace the node by its single child.
>     - Delete a degree 2 node: 1. Replace the node by the largest one in its left subtree or the smallest one in its right subtree. 2. Delete the replacing node from the subtree.
>       > [!note]-
>       > If there are not many deletions, then lazy deletion may be employed: add a flag field to each node, to mark if a node is active or is deleted.Â  Therefore we can delete a node without actually freeing the space of that node.Â  If a deleted key is reinserted, we wonâ€™t have to call malloc again.
> - Tree -> Threaded Binary Tree
>   1. write the pre-order/in-order/post-order traversals
>   2. replace the null node with its predecessor/successor(å‰é©±/åé©±)

1. full binary tree çš„èŠ‚ç‚¹æ•°é‡ä¸º$2^n$
2. If a general treeÂ TÂ is converted into a binary treeÂ BT, then which of the followingÂ BTÂ traversals gives the same sequence as that of the post-order traversal ofÂ T? `In-order traversal`

# Priority Queue ä¼˜å…ˆé˜Ÿåˆ—

A min/max heap is a complete binary tree that is also a min tree.

> [!summary]+ Operation
>
> - **Percolate up/down**
> - Insert
> - Delete min/max
> - **min heap <-> max heap**

```c
// a random array -> heap
for (int i = N / 2; i >= 0; --i)
 PercolateDown(i);
```

# The Disjoint Set ADT ä¸ç›¸äº¤é›†

> [!quote]+ Conception
>
> - Equivalence Relations
>   A relation R is defined on a set $S$ if for every pair of elements (a, b), $a, b \in S$, $a \ R \ b$ is either true or false.Â  If $a \ R \ b$ is true, then we say that a is related to b.
>   Properties:
>   1. (Reflexive è‡ªåæ€§) $a \ R \ a$, for all $a \in S$.
>   2. (Symmetric å¯¹ç§°æ€§) $a \ R \ b$ if and only if $b \ R \ a$.
>   3. (Transitive ä¼ é€’æ€§) $a \ R \ b$ and $b \ R \ c$ that $a \ R \ c$
> - disjoint

> [!summary]+ Operation
>
> - Union and Find -> Union-find
> - Union-by-Size
> - Union-by-Height
> - Path Compression

# Graph å›¾

> [!quote] Definitions
> $G(V, E)$
> Undirected graph
> Directed graph
> Complete graph: a graph that has the maximum number of edges
>
> Simple path ::= $v_{i1}, v_{i2}, \cdots, v_{in}$ are distinct
> Cycle
> connected: - vi and vj in an undirected G are connected if there is a path from vi to vj (and hence there is also a path from vj to vi) - An undirected graph G is connected if every pair of distinct vi and vj are connected

## Wrong Problem Collection é”™é¢˜é›†

1. If an undirected graph G = (V, E) contains 10 vertices. Then to guarantee that G is connected **in any cases**, there has to be at least \_\_ edges.
   å¦‚æœå‰ V-1 ä¸ªèŠ‚ç‚¹æ˜¯å…¨è¿æ¥çš„ï¼Œé‚£ä¹ˆä»»æ„åŠ å…¥ 1 edge æ‰å¯ä»¥ guarantee that G is connected.
   37 `YES` | 9 `NO`
2. In a binary search tree which contains several integer keys including 4, 5, and 6, if 4 and 6 are on the same level, then 5 must be their parent.
   `NO` counterexample: 4 <- 3 <- 5 -> 7 -> 6
3. In a complete binary tree with 1102 nodes, there must be \_\_ leaf nodes.Â (6 åˆ†)
   å€’æ•°ç¬¬äºŒæ’çš„å¤§éƒ¨åˆ†èŠ‚ç‚¹ä¹Ÿæ˜¯å¶å­èŠ‚ç‚¹
   1. 79
   2. 551
   3. 1063
   4. cannot be determined
4. The recurrent equations for the time complexities of programs P1 and P2 are:

- P1:Â T(1)=1,Â T(N)=T(N/2)+1;
- P2:Â T(1)=1,Â T(N)=2T(N/2)+1;

Then the correct conclusion about their time complexities is:

2. O(logN)Â for P1, andÂ O(N)Â for P2
   <https://zhu45.org/posts/2017/Mar/26/maw-chapter-6-priority-queues-heaps-writing-questions/>

If aÂ d-heap is stored as an array, for an entry located in positionÂ i, the parent, the first child and the last child are at:
A. âŒˆ(i+dâˆ’2)/dâŒ‰,Â (iâˆ’2)d+2, andÂ (iâˆ’1)d+1
B. âŒˆ(i+dâˆ’1)/dâŒ‰,Â (iâˆ’2)d+1, andÂ (iâˆ’1)d
C. âŒŠ(i+dâˆ’2)/dâŒ‹,Â (iâˆ’1)d+2, andÂ id+1
D. âŒŠ(i+dâˆ’1)/dâŒ‹,Â (iâˆ’1)d+1, andÂ id

1. For a graph, if each vertex has an even degree or only two vertexes have odd degree, we can find a cycle that visits every edge exactly once
   è¦æ±‚å›¾æ˜¯è”é€šçš„

![[Pasted image 20240509233731.png]]

> [!tip]
>
> 1. If time complexity with adjacency list is $O(|E| + |V|)$, then it's $O(|V|^2)$ for adjacency matrix(Just think the graph is dense, then we have $|E| \approx |V|^2$)
> 2. For recursive sorting algorithm, we can use recursive relations to analyse time complexity

## Representation of graph

- adjacency matrix: space complexity$O(|V^2|)$ suitable for dense graph
- adjacency list: $O(|E| + |V|)$ suitable for sparse graph

Question: The vertex is name(string) rather than number
Solutions:

- use _hash table_
- string array(not recommend)
- use pointer to hash table (lose purity)

## Topological sort (DAG) æ‹“æ‰‘æ’åº

time complexity: $O(|V|^2)$ bad point: sequential scan for Indegree array
optimize: use _stack_ or _queue_ to store vertices whose indegree is 0 and adjacency list -> $O(|E| + |V|)$
é€†åºæ‹“æ‰‘æ’åº â“

## The shortest path algorithm æœ€çŸ­è·¯å¾„ç®—æ³•

- unweighted: BFS(breadth-first search, like lever-order traversal to tree): use queue and adjacency list -> $O(|E| + |V|)$
- weighted positive edge: Dijkstra algorithm
  1. `findMin`(simply scan the table) $O(|V|^2)$ + `update` $O(|E|)$ = $O($|V|^2)
      PS: å¯ä»¥é€šè¿‡è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„ä¸Šä¸ªèŠ‚ç‚¹æ¥è®°å½•è·¯å¾„ï¼Œç”¨é€’å½’æ¥æ˜¾ç¤º
  2. keep distance in a priority queue(can optimize with pairing heap):
      - `deleteMin` $O(|E|\ log|V|)$ + `decrease` $O(|V|\ log{V})$ = $O(|E|\ log|V|)$
      - decrease -> insert xxx ğŸ‘€
  3. use Fibonacci heap: $O(|E| + |V|\ log|V|)$ ğŸ‘€
- weighted edge(maybe negative): combine weighted and unweighted algorithm: If a vertex is updated, then put it into queue (if it's not in queue), then dequeue, update and continue: $O(|E||V|)$
  If there is negative-cost cycle(è´Ÿå€¼åœˆ) in the graph, stop the algorithm if any vertex has been dequeued for $|V| + 1$ times to ensure the program will end. â“

If the graph is acyclic, we can use topological sort to improve Dijkstra algorithm $O(|E| + |V|)$
Application: critical path analysis(activity-node graph -> event-node graph: analysis based on edge rather than vertex) find the longest path and obtain earliest/latest completion time of every node, then obtain the slack time(æ¾å¼›æ—¶é—´), which represent how long one activity can be delayed without delaying the overall completion
all-pairs shortest path problem: use single-source algorithm for $V$ times

## Minimum spanning tree

### Prim

### Kruskal

## Strong Connected Component(SCC) å¼ºè¿é€šåˆ†é‡

### Kosaraju Algorithm

# Sort

![[Pasted image 20240521015228.png]]

stability: a sorting algorithm is said to be stable if two objects with equal keys appear in the same order in sorted output as they appear in the input data set.

- stable sort algorithm: Bubble Sort, Insertion Sort, Merge Sort, Count Sort(?)
- un stable sort algorithm: Heap Sort, Quick Sort, Shell Sort

Q: which sort algorithm need extra space?
A: Merge Sort

During the sorting, processing every element which is not yet at its final position is called a "run"

Q: Which sort algorithm will be slowed down if we store the elements in a linked structure instead of a sequential structure?
A: Heap Sort, Shell Sort, Quick Sort
Merge Sort?

## Insertion Sort æ’å…¥æ’åº

```c
// Insertion Sort æ’å…¥æ’åº
void InsertionSort(ElementType A[], int N) 
{
    // è¿™æ ·äº¤æ¢èµ‹å€¼çš„æ¬¡æ•°ä¼šå‡åŠ
    int j, P;
    ElementType Tmp;
    for (P = 1; P < N; P++) {
        Tmp = A[p];
        for (j = P; j > 0 && A[j-1] > Tmp; j--)
            A[j] = A[j-1];
        A[j] = Tmp;
    }
}
```

Time Complexity: worst $O(N^2)$ best $O(N)$
If the sequence is almost sorted, then insertion sort will run fast

## Shell Sort å¸Œå°”æ’åº

shell sort is also called diminishing increment sort(ç¼©å°å¢é‡æ’åº)
increment sequence(å¢é‡åºåˆ—)

```

```

Question: give two sequence and ask the increment of shell sort
Method: ä»£å…¥ä¸åŒçš„ increment çœ‹è¿™ä¸ª increment çš„ sequence æ˜¯ä¸æ˜¯ sortedï¼Œå†æ¯”è¾ƒä¸¤åºåˆ—ä¸­å…ƒç´ æ˜¯å¦ç›¸åŒ

## Heap Sort

```c
void HeapSort(ElementType A[], int N)
{
 for (int i = N / 2; i >= 0; i--)
  PercDown(A, i, N);
 for (int i = N - 1; i > 0; i--) {
  Swap(&A[0], &A[i]);
  PercDown(A, 0, i);
 }

}
```

## Merge Sort

divide-and-conquer strategy

```c
void MergeSort(ElementType A[], ElementType TmpArray[], int Left, int Right)
{
 if (Left < Right) {
  int Center = (Left + Right) / 2;
  MergeSort(A, TmpArray, Left, Center);
  MergeSort(A, TmpArray, Center + 1, Right);
  Merge(A, TmpArray, Left, Center + 1, Right)
 }
}
```

space complexity: $O(NlogN)$ - A å’Œ TmpArray æ‹·è´ -> é€’å½’äº¤æ›¿å±‚æ¬¡ - save space: A[] \* max + Temp[] -> Final[], it can also be understood as a number in multiple digits, AT, ç¬¬äºŒä½è¡¨ç¤º A[i], ç¬¬ä¸€ä½è¡¨ç¤º Temp[i]
time complexity: $O(NlogN)$ (derived from recursive relation)

Comment: It is widely used for external sorting, where random access can be very, very expensive compared to sequential access.

éš¾ç”¨äºè´®å­˜æ’åº
variant: implementation without recursion

## Quick Sort å¿«é€Ÿæ’åº

The fastest known sorting algorithm in practice

1. pick pivot(æ¢çº½å…ƒ)
   - a wrong & uninformed way: choose the first element (ğŸ¤”bad: If the input is presorted, then quick sort will take $O(N)^2$ time to do nothing)
   - a safe maneuver: randomly choose pivot (ğŸ¤”bad: random number generation is generally expensive)
   - Median-of-Three Partitioning(ä¸‰æ•°ä¸­å€¼åˆ†å‰²æ³•): pick three elements randomly and using the median of these three as pivot
     The minimum and maximum number wind up in the correct place, which act as a sentinel
2. partition (two schemes) 1. ä»å·¦åˆ°å³æ‰«å¹¶è®¾ç½®ä¸€ä¸ªåˆ†ç•Œçº¿ 2. å·¦å³ä¸¤è¾¹ä¸€èµ·æ‰«ï¼Œæœ‰ä¸ç¬¦åˆçš„å°±äº’æ¢
   For very small arrays($N \le 20$), use insertion sort rather than quick sort
   Time complexity: best $O(NlogN)$(analysis is same as merge sort) worst $O(N^2)$

Comment: - Quicksort is probably more effective for datasets that fit in memory. For larger data sets it proves to be inefficient so algorithms like merge sort are preferred in that case.

Application: find k-th maximum number å¯ä»¥ç¡®å®š k å¤§æ•°åœ¨ pivot çš„å·¦å³æ¥ç¼©å°åŒºé—´

## Bucket Sort

## Radix Sort

<https://www.geeksforgeeks.org/msd-most-significant-digit-radix-sort/>

- Least Signification Digit (LSD) radix sort
- Most Signification Digit (MSD) radix sort

## Counting Sort

## Questions

> [!question] Common question
> åˆ¤æ–­æŸä¸ªåºåˆ—æ˜¯æŸä¸ªæ’åºç¬¬ k æ¬¡æ‰§è¡Œå®Œçš„ç»“æœ
>
> - quick sort: çœ‹ sequence ä¸­æœ‰å‡ ä¸ª pivotï¼Œæ­£å¸¸æ¥è¯´åº”è¯¥æœ‰$2^k - 1$ï¼ˆå‡†ç¡®æ¥è¯´åº”è¯¥æ˜¯å¤§äºç­‰äºï¼‰ï¼Œä½†æ˜¯å¦‚æœæŸä¸ª pivot æ˜¯æœ€å‰/åé¢çš„åˆ™ä¼šå°‘ 1 ä¸ª
> - LSD/MSD radix sort: ç›´æ¥å»çœ‹é€‰é¡¹ä¸­åºåˆ—çš„å/å‰ k ä½æ˜¯ä¸æ˜¯ä»å°åˆ°å¤§ï¼ˆæˆ–è€…è‡ªå·±å…ˆå†™å‡ºæ¥ä¹Ÿè¡Œï¼‰

During the sorting, processing every element which is not yet at its final position is called a "run". Which of the following cannot be the result after the second run of quicksort?

A. 5, 2, 16, 12, 28, 60, 32, 72
B. 2, 16, 5, 28, 12, 60, 32, 72
C. 2, 12, 16, 5, 28, 32, 72, 60
D. 5, 2, 12, 28, 16, 32, 72, 60

During the sorting, processing every element which is not yet at its final position is called a "run". To sort a list of integers using quick sort, it may reduce the total number of recursions by processing the small partion first in each run.

> F. The number of recursions is independent of which sequence is processed first

![[Pasted image 20240521020705.png]]

# Hash

# Bonus

## LRU(Lease Recent Used)

æ”¯æŒ get å’Œ put æ“ä½œ
[å¾…åšçš„ LFU](https://leetcode.cn/problems/lfu-cache/description/)
